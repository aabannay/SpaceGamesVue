{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMhGLweBKrfB1EEdQueKWBS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"id":"E2eiqecuhncR","executionInfo":{"status":"ok","timestamp":1741855873055,"user_tz":-180,"elapsed":3497,"user":{"displayName":"Osamah 1995","userId":"07866273701012002264"}}},"outputs":[],"source":["import cv2\n","import numpy as np\n","import torch\n"]},{"cell_type":"code","source":["# Choose model type: \"DPT_Large\" (more accurate) or \"MiDaS_small\" (faster)\n","model_type = \"DPT_Large\"\n","\n","# Load the MiDaS model from PyTorch Hub\n","midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","midas.to(device)\n","midas.eval()\n","\n","# Load the appropriate transforms for the chosen model\n","midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n","if model_type in [\"DPT_Large\", \"DPT_Hybrid\"]:\n","    transform = midas_transforms.dpt_transform\n","else:\n","    transform = midas_transforms.small_transform\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bNYOEA3Nh8ac","executionInfo":{"status":"ok","timestamp":1741856151796,"user_tz":-180,"elapsed":11076,"user":{"displayName":"Osamah 1995","userId":"07866273701012002264"}},"outputId":"05f8f5b0-eee6-43d4-a731-8bd5cbd2f1f3"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n","Downloading: \"https://github.com/isl-org/MiDaS/releases/download/v3/dpt_large_384.pt\" to /root/.cache/torch/hub/checkpoints/dpt_large_384.pt\n","100%|██████████| 1.28G/1.28G [00:02<00:00, 488MB/s]\n","Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"]}]},{"cell_type":"code","source":["# Define maximum disparity (pixel shift) for closest objects\n","max_disp = 20\n","\n","# Open the input video\n","input_video = \"input.mp4\"\n","cap = cv2.VideoCapture(input_video)\n","if not cap.isOpened():\n","    raise ValueError(\"Error opening video file\")\n","\n","# Retrieve video properties\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# Prepare a VideoWriter for the output (side-by-side) video\n","output_video = \"output_stereo.mp4\"\n","fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","# Output width is doubled (left + right view)\n","out = cv2.VideoWriter(output_video, fourcc, fps, (width * 2, height))\n"],"metadata":{"id":"hb3IREP_igam","executionInfo":{"status":"ok","timestamp":1741856171072,"user_tz":-180,"elapsed":38,"user":{"displayName":"Osamah 1995","userId":"07866273701012002264"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","print(f\"Processing {frame_count} frames...\")\n","\n","frame_index = 0\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Convert the frame from BGR to RGB\n","    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","\n","    # Preprocess image and move to GPU\n","    input_batch = transform(img_rgb).to(device)\n","\n","    # Estimate depth with the MiDaS model\n","    with torch.no_grad():\n","        prediction = midas(input_batch)\n","\n","    # Resize depth prediction to original frame dimensions\n","    prediction_resized = torch.nn.functional.interpolate(\n","        prediction.unsqueeze(1),\n","        size=(height, width),\n","        mode=\"bicubic\",\n","        align_corners=False\n","    ).squeeze().cpu().numpy()\n","\n","    # Normalize the depth map to range [0, 1]\n","    depth_min = prediction_resized.min()\n","    depth_max = prediction_resized.max()\n","    normalized_depth = (prediction_resized - depth_min) / (depth_max - depth_min + 1e-8)\n","\n","    # Compute disparity: closer objects (lower depth) get a higher disparity.\n","    # We use (1 - normalized_depth) so that nearer objects have larger disparity.\n","    disparity = max_disp * (1 - normalized_depth)\n","    disparity = disparity.astype(np.float32)\n","\n","    # Create a meshgrid for pixel coordinates\n","    xx, yy = np.meshgrid(np.arange(width), np.arange(height))\n","    # For the right view, shift pixels horizontally by subtracting the disparity\n","    # (simulating the perspective of a right-offset camera)\n","    map_x = (xx - disparity).astype(np.float32)\n","    map_y = yy.astype(np.float32)\n","\n","    # Warp the original frame to generate the right view using the computed mapping\n","    right_view = cv2.remap(frame, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n","\n","    # Concatenate the original frame (left view) and the warped frame (right view) side-by-side\n","    stereo_frame = np.concatenate((frame, right_view), axis=1)\n","\n","    # Write the stereoscopic frame to the output video\n","    out.write(stereo_frame)\n","\n","    frame_index += 1\n","    if frame_index % 10 == 0:\n","        print(f\"Processed {frame_index}/{frame_count} frames\", end='\\r')\n","\n","cap.release()\n","out.release()\n","print(\"\\nStereoscopic conversion complete!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fOQX8BYjVu8","executionInfo":{"status":"ok","timestamp":1741856291705,"user_tz":-180,"elapsed":117324,"user":{"displayName":"Osamah 1995","userId":"07866273701012002264"}},"outputId":"e85deae4-7fc6-4d3e-ac00-9c5e39e4f78f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing 1238 frames...\n","\n","Stereoscopic conversion complete!\n"]}]}]}